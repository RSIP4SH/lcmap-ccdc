{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz     import assoc\n",
    "from cytoolz     import assoc_in\n",
    "from cytoolz     import first\n",
    "from cytoolz     import get\n",
    "from cytoolz     import get_in\n",
    "from cytoolz     import merge\n",
    "from cytoolz     import second\n",
    "from cytoolz     import take\n",
    "from cytoolz     import thread_first\n",
    "from cytoolz     import thread_last\n",
    "from firebird    import ARD\n",
    "from firebird    import grid\n",
    "from firebird    import pyccd\n",
    "from firebird    import timeseries\n",
    "from functools   import partial\n",
    "\n",
    "from importlib   import reload\n",
    "from merlin      import functions as f\n",
    "from pyspark.sql import functions as ff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "import firebird\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession(pyspark.SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build timeseries & run PyCCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -2094585 \n",
    "y = 1952805\n",
    "\n",
    "tile = grid.tile(x=x, y=y, cfg=ARD)\n",
    "\n",
    "chips = take(1, tile.get('chips'))\n",
    "\n",
    "ids = timeseries.ids(ctx=ss.sparkContext, chips=chips)\n",
    "\n",
    "ard = timeseries.rdd(ctx=ss.sparkContext, ids=ids, acquired='1980-01-01/2017-01-01', cfg=firebird.ARD, name='ard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = pyccd.rdd(ctx=ss.sparkContext, timeseries=ard)\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve matching aux data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = timeseries.aux(acquired='1980-01-01/2017-01-01', ctx=ss.sparkContext, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.freqItems(cols=['trends']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.filter(aux.trends[0] != 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.first(aux.trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.trends[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = aux.filter('trends[0] NOT IN (1, 9)').select(aux.chipx, aux.chipy).distinct()\n",
    "a2 = aux.filter('trends[0] NOT IN (2, 9)').select(aux.chipx, aux.chipy).distinct()\n",
    "a3 = a1.join(a2, on=(['chipx', 'chipy']), how='inner')\n",
    "a3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get('snap_fn', firebird.AUX)\n",
    "#firebird.AUX.get('snap_fn')(x=-2115585, y=1964805)\n",
    "g(x=-2115585, y=1964805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = get_in(['tile', 'proj-pt'], g(x=x, y=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results off cluster onto local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it = aux.rdd.toLocalIterator()\n",
    "#[print(i) for i in it]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ff.udf(returnType=types.StringType())\n",
    "def myudf(trends, mpw, aspect, dem, posidex):\n",
    "    return 'seperate'\n",
    "\n",
    "\n",
    "@ff.udf(returnType=types.StringType())\n",
    "def myudf2(*args, **kwargs):\n",
    "    \n",
    "    return str(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(chipx=-2115585, chipy=1964805, x=-2114145, y=1964715, dates=[730332], dem=None, trends=[0], aspect=None, posidex=None, slope=None, mpw=None, happiness='([0], None)')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=['trends', 'mpw']\n",
    "aux.withColumn('happiness', myudf2(*cols)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out how to select multiple columns programatically\n",
    "ss.createDataFrame(aux.rdd.map(lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([9.0, 1.0, 2.0, 3.0, 0.0, 1.0, 2.0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.dense(list(map(lambda x: first(x) if type(x) in [tuple, list, set] else x, [[9, 9, 9], 1, 2, 3, 0, 1, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method alias in module pyspark.sql.dataframe:\n",
      "\n",
      "alias(alias) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` with an alias set.\n",
      "    \n",
      "    >>> from pyspark.sql.functions import *\n",
      "    >>> df_as1 = df.alias(\"df_as1\")\n",
      "    >>> df_as2 = df.alias(\"df_as2\")\n",
      "    >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "    >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n",
      "    [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(aux.wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
