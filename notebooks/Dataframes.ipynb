{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz     import assoc\n",
    "from cytoolz     import assoc_in\n",
    "from cytoolz     import first\n",
    "from cytoolz     import get\n",
    "from cytoolz     import get_in\n",
    "from cytoolz     import merge\n",
    "from cytoolz     import second\n",
    "from cytoolz     import take\n",
    "from cytoolz     import thread_first\n",
    "from cytoolz     import thread_last\n",
    "from firebird    import ARD\n",
    "from firebird    import grid\n",
    "from firebird    import pyccd\n",
    "from firebird    import timeseries\n",
    "from functools   import partial\n",
    "\n",
    "from importlib   import reload\n",
    "from merlin      import functions as f\n",
    "from pyspark.sql import functions as ff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "import firebird\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession(pyspark.SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build timeseries & run PyCCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -2094585 \n",
    "y = 1952805\n",
    "\n",
    "tile = grid.tile(x=x, y=y, cfg=ARD)\n",
    "\n",
    "chips = take(1, tile.get('chips'))\n",
    "\n",
    "ids = timeseries.ids(ctx=ss.sparkContext, chips=chips)\n",
    "\n",
    "ard = timeseries.rdd(ctx=ss.sparkContext, ids=ids, acquired='1980-01-01/2017-01-01', cfg=firebird.ARD, name='ard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = pyccd.rdd(ctx=ss.sparkContext, timeseries=ard)\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve matching aux data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = timeseries.aux(acquired='1980-01-01/2017-01-01', ctx=ss.sparkContext, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.freqItems(cols=['trends']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.filter(aux.trends[0] != 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.first(aux.trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.trends[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = aux.filter('trends[0] NOT IN (1, 9)').select(aux.chipx, aux.chipy).distinct()\n",
    "a2 = aux.filter('trends[0] NOT IN (2, 9)').select(aux.chipx, aux.chipy).distinct()\n",
    "a3 = a1.join(a2, on=(['chipx', 'chipy']), how='inner')\n",
    "a3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get('snap_fn', firebird.AUX)\n",
    "#firebird.AUX.get('snap_fn')(x=-2115585, y=1964805)\n",
    "g(x=-2115585, y=1964805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = get_in(['tile', 'proj-pt'], g(x=x, y=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results off cluster onto local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it = aux.rdd.toLocalIterator()\n",
    "#[print(i) for i in it]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ff.udf(returnType=types.StringType())\n",
    "def myudf(trends, mpw, aspect, dem, posidex):\n",
    "    return 'seperate'\n",
    "\n",
    "\n",
    "@ff.udf(returnType=types.StringType())\n",
    "def myudf2(*args, **kwargs):\n",
    "    \n",
    "    return str(args)\n",
    "\n",
    "from pyspark.ml.linalg import Vector, VectorUDT\n",
    "\n",
    "@ff.udf(returnType=VectorUDT())\n",
    "def densify(data):\n",
    "    #return Vectors.dense(data)\n",
    "    return Vectors.dense(2)\n",
    "\n",
    "@ff.udf(returnType=types.IntegerType())\n",
    "def small(data):\n",
    "    return data % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(chipx=-2115585, chipy=1964805, x=-2114145, y=1964715, dates=[730332], dem=None, trends=[0], aspect=None, posidex=None, slope=None, mpw=None, happiness='([0], None)')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=['trends', 'mpw']\n",
    "aux.withColumn('happiness', myudf2(*cols)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out how to select multiple columns programatically\n",
    "ss.createDataFrame(aux.rdd.map(lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vd = Vectors.dense(list(map(lambda x: fiyst(x) if type(x) in [tuple, list, set] else x, [[9, 9, 9], 1, 2, 3, 0, 1, 2])))\n",
    "\n",
    "aa = aux.withColumn('features', densify(aux.trends))\n",
    "bb = aa.withColumn('label', small(aux.chipx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumnRenamed in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumnRenamed(existing, new) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "    This is a no-op if schema doesn't contain the given column name.\n",
      "    \n",
      "    :param existing: string, name of the existing column to rename.\n",
      "    :param col: string, new name of the column.\n",
      "    \n",
      "    >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "    [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "r = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = r.fit(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.util.JavaMLWriter at 0x7f913c964b38>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-100 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chipx', 'int'),\n",
       " ('chipy', 'int'),\n",
       " ('x', 'int'),\n",
       " ('y', 'int'),\n",
       " ('dates', 'array<int>'),\n",
       " ('dem', 'array<float>'),\n",
       " ('trends', 'array<int>'),\n",
       " ('aspect', 'array<int>'),\n",
       " ('posidex', 'array<float>'),\n",
       " ('slope', 'array<float>'),\n",
       " ('mpw', 'array<int>'),\n",
       " ('features', 'vector'),\n",
       " ('label', 'int')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function translate in module pyspark.sql.functions:\n",
      "\n",
      "translate(srcCol, matching, replace)\n",
      "    A function translate any character in the `srcCol` by a character in `matching`.\n",
      "    The characters in `replace` is corresponding to the characters in `matching`.\n",
      "    The translate will happen when any character in the string matching with the character\n",
      "    in the `matching`.\n",
      "    \n",
      "    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "    ...     .alias('r')).collect()\n",
      "    [Row(r='1a2s3ae')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
